{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d316b0a3-0158-40e3-8a37-6781a3c74d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import read_avazu, read_avazu_ml, read_criteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4bebba-020f-4d3e-924a-48385d616fd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading avazu...\n",
      "finish reading avazu.\n",
      "features: hour\n",
      "mix: 0 max: 239\n",
      "features: C1\n",
      "mix: 0 max: 6\n",
      "features: banner_pos\n",
      "mix: 0 max: 6\n",
      "features: site_id\n",
      "mix: 0 max: 4736\n",
      "features: site_domain\n",
      "mix: 0 max: 7744\n",
      "features: site_category\n",
      "mix: 0 max: 25\n",
      "features: app_id\n",
      "mix: 0 max: 8551\n",
      "features: app_domain\n",
      "mix: 0 max: 558\n",
      "features: app_category\n",
      "mix: 0 max: 35\n",
      "features: device_id\n",
      "mix: 0 max: 2686407\n",
      "features: device_ip\n",
      "mix: 0 max: 6729485\n",
      "features: device_model\n",
      "mix: 0 max: 8250\n",
      "features: device_type\n",
      "mix: 0 max: 4\n",
      "features: device_conn_type\n",
      "mix: 0 max: 3\n",
      "features: C14\n",
      "mix: 0 max: 2625\n",
      "features: C15\n",
      "mix: 0 max: 7\n",
      "features: C16\n",
      "mix: 0 max: 8\n",
      "features: C17\n",
      "mix: 0 max: 434\n",
      "features: C18\n",
      "mix: 0 max: 3\n",
      "features: C19\n",
      "mix: 0 max: 67\n",
      "features: C20\n",
      "mix: 0 max: 171\n",
      "features: C21\n",
      "mix: 0 max: 59\n",
      "[240, 7, 7, 4737, 7745, 26, 8552, 559, 36, 2686408, 6729486, 8251, 5, 4, 2626, 8, 9, 435, 4, 68, 172, 60]\n"
     ]
    }
   ],
   "source": [
    "features, label, train_dataloader, val_dataloader, test_dataloader, unique_values = read_avazu('/root/autodl-tmp/', batch_size=4096, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42963508-acb0-4ccc-8662-a9e709935306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self, input_dim, unique_values, embed_dims=[16, 8], dropout=0.2, output_layer=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(sum(unique_values), 16)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "        self.offsets = np.array((0, *np.cumsum(unique_values)[:-1]))\n",
    "        \n",
    "        layers = list()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        self.out_layer = output_layer\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "            self.mlps.append(nn.Sequential(*layers))\n",
    "            layers = list()\n",
    "        if self.out_layer:\n",
    "            self.out = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
    "        \"\"\"\n",
    "        x = self.embedding(x + x.new_tensor(self.offsets).unsqueeze(0))\n",
    "        b = x.shape[0]\n",
    "        x = x.reshape(b, -1)\n",
    "        for layer in self.mlps:\n",
    "            x = layer(x)\n",
    "        if self.out_layer:\n",
    "            x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda1e9b6-6e95-44b4-aae0-77af52a214d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"Logistic Regression Module. It is the one Non-linear \n",
    "    transformation for input feature.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input size of Linear module.\n",
    "        sigmoid (bool): whether to add sigmoid function before output.\n",
    "\n",
    "    Shape:\n",
    "        - Input: `(batch_size, input_dim)`\n",
    "        - Output: `(batch_size, 1)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, sigmoid=False):\n",
    "        super().__init__()\n",
    "        self.sigmoid = sigmoid\n",
    "        self.fc = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sigmoid:\n",
    "            return torch.sigmoid(self.fc(x))\n",
    "        else:\n",
    "            return self.fc(x)\n",
    "class CrossNetwork(nn.Module):\n",
    "    \"\"\"CrossNetwork  mentioned in the DCN paper.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dim of input tensor\n",
    "    \n",
    "    Shape:\n",
    "        - Input: `(batch_size, *)`\n",
    "        - Output: `(batch_size, *)`\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.w = torch.nn.ModuleList([torch.nn.Linear(input_dim, 1, bias=False) for _ in range(num_layers)])\n",
    "        self.b = torch.nn.ParameterList([torch.nn.Parameter(torch.zeros((input_dim,))) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            xw = self.w[i](x)\n",
    "            x = x0 * xw + self.b[i] + x\n",
    "        return x\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi Layer Perceptron Module, it is the most widely used module for \n",
    "    learning feature. Note we default add `BatchNorm1d` and `Activation` \n",
    "    `Dropout` for each `Linear` Module.\n",
    "\n",
    "    Args:\n",
    "        input dim (int): input size of the first Linear Layer.\n",
    "        output_layer (bool): whether this MLP module is the output layer. If `True`, then append one Linear(*,1) module. \n",
    "        dims (list): output size of Linear Layer (default=[]).\n",
    "        dropout (float): probability of an element to be zeroed (default = 0.5).\n",
    "        activation (str): the activation function, support `[sigmoid, relu, prelu, dice, softmax]` (default='relu').\n",
    "\n",
    "    Shape:\n",
    "        - Input: `(batch_size, input_dim)`\n",
    "        - Output: `(batch_size, 1)` or `(batch_size, dims[-1])`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_layer=True, dims=[16,16], dropout=0, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        if dims is None:\n",
    "            dims = []\n",
    "        layers = list()\n",
    "        for i_dim in dims:\n",
    "            layers.append(nn.Linear(input_dim, i_dim))\n",
    "            layers.append(nn.BatchNorm1d(i_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            input_dim = i_dim\n",
    "        if output_layer:\n",
    "            layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "class DCN(torch.nn.Module):\n",
    "    \"\"\"Deep & Cross Network\n",
    "\n",
    "    Args:\n",
    "        features (list[Feature Class]): training by the whole module.\n",
    "        mlp_params (dict): the params of the last MLP module, keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_values, n_cross_layers):\n",
    "        super().__init__()\n",
    "        self.dims = 16 * len(unique_values)\n",
    "        self.embedding = torch.nn.Embedding(sum(unique_values), 16)\n",
    "        self.cn = CrossNetwork(self.dims, n_cross_layers)\n",
    "        self.mlp = MLP(self.dims, output_layer=False)\n",
    "        self.linear = LR(self.dims + 16)\n",
    "        self.offsets = np.array((0, *np.cumsum(unique_values)[:-1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        embed_x = self.embedding(x + x.new_tensor(self.offsets).unsqueeze(0)).reshape(b,-1)\n",
    "        cn_out = self.cn(embed_x)\n",
    "        mlp_out = self.mlp(embed_x)\n",
    "        x_stack = torch.cat([cn_out, mlp_out], dim=1)\n",
    "        y = self.linear(x_stack)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254c7f14-df70-4a1e-bbff-8ef95b2de2f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dde47d92ec740938d3c1c2e9bc2d1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/7897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339d1d4c9a9a41f78dcd5ad968834354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  auc  0.7796314166598058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40af6b866604f15a4d33aa8e33dfde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/7897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7f3d46cd5846dcb5f293b9d43c43e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  auc  0.78402917347738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f524bbdbb92b466ea448e0dab1a1e85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/7897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c548dd906a4c569beff0cceb7d6a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  2  auc  0.7819189192129827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c5a9050ef945a58ff9e6a8eb12e267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/7897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27ca2c8b2cc42769a9d8d954fd92bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  3  auc  0.78032092596661\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8df59fe8b694dfdb95a11395085e191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/7897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2876e76995b7455981f4225e97edf858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  4  auc  0.779318800044741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963c4d39f148417b9c846e7e0d5bcb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/7897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aed6adc09444c3e92760aafbed82720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  5  auc  0.7789293513186053\n",
      "early stop. best auc: 0.78402917347738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd82aa75fa7f4762a9299701a2413829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.7844399425526216\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "# train\n",
    "device = torch.device('cuda')\n",
    "model = mlp(input_dim=len(features)*16, unique_values=unique_values).to(device)\n",
    "# model = DCN(unique_values, 2).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "criterion = torch.nn.BCELoss()\n",
    "best_auc = 0.0\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tk0 = tqdm(train_dataloader, desc=\"train\", smoothing=0, mininterval=1.0)\n",
    "    for i, (x, y) in enumerate(tk0):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y.float().reshape(-1, 1))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            tk0.set_postfix(loss=total_loss / 100)\n",
    "            total_loss = 0\n",
    "    model.eval()\n",
    "    targets, predicts = list(), list()\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(val_dataloader, desc=\"validation\", smoothing=0, mininterval=1.0)\n",
    "        for i, (x, y) in enumerate(tk0):\n",
    "            x = x.to(device)\n",
    "            # x_dict = {k: v.to(self.device) for k, v in x_dict.items()}\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x) # current_epoch=None means not in training mode\n",
    "            targets.extend(y.tolist())\n",
    "            predicts.extend(y_pred.tolist())\n",
    "    auc =  roc_auc_score(targets, predicts)\n",
    "    print('epoch ',epoch, ' auc ', auc)\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience > 3:\n",
    "            print('early stop. best auc:', best_auc)\n",
    "            break\n",
    "    \n",
    "\n",
    "# test\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "targets, predicts = list(), list()\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(test_dataloader, desc=\"test\", smoothing=0, mininterval=1.0)\n",
    "    for i, (x, y) in enumerate(tk0):\n",
    "        x = x.to(device)\n",
    "        # x_dict = {k: v.to(self.device) for k, v in x_dict.items()}\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x) # current_epoch=None means not in training mode\n",
    "        targets.extend(y.tolist())\n",
    "        predicts.extend(y_pred.tolist())\n",
    "auc =  roc_auc_score(targets, predicts)\n",
    "print('test auc:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4fee9-5f99-4c8d-9de4-1124cb823959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf8428aa180ee23632ed7df20f7a595edda7c60e668686876baf89d702ea1cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
